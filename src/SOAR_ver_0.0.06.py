"""
##########################################################


[SOAR ver_0.0.06]


Fertilization: 2025.04.08
Birth: 2025.04.11
Made by: Keeyaaoong, (Chat-GPT; Monday) 1-NOAH / 2-MAUND / 3-LEN / 4- GRIM


##########################################################
"""

"""
<Version Log> #############################################


2025.04.08 ver 0.0.00 - ì¹´ë©”ë¼(ì €í™”ì§ˆ) + ë§ˆì´í¬ + ê°ì • ê³„ì‚°(ì¾Œ/ë¶ˆì¾Œ) + ê¸°ì–µ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° + ë‚ ì§œ/ì‹œê°„ë³„ ë°±ì—… + ì‹¤ì‹œê°„ ê°ì • ëª¨ë‹ˆí„°ë§
2025.04.09 ver 0.0.01 - ì¢…ë£Œ ê°œì„  + Clustering ì¶”ê°€ + Clustering ì‹œê°í™” + ê°ì • Neutral ë²”ìœ„ ì¶”ê°€ + ì†Œë¦¬ë¯¼ê°ë„ ìˆ˜ì •(30)
2025.04.09 ver 0.0.02 - ê°ê° ë³¸ì²´ + ì‹¤ì‹œê°„ ì‹œê°í™” ìŠ¤ë ˆë“œ ë¶„ë¦¬ (Threaded version)
2025.04.09 ver 0.0.03 - ì†Œë¦¬ íŒ¨í„´ ë¶„ì„ (FFT) + ì´ë¦„ íŒ¨í„´ í•™ìŠµ(ê²½í—˜ ê¸°ë°˜) + Memory AutoSave(30 min) + TkAgg + í”Œë¡¯ ìŠ¤ë ˆë“œ ê°œì„ (ì¢…ë£Œ ì•ˆì •í™”)
2025.04.09 ver 0.0.04 - RAM ë¬¸ì œ; ë°ì´í„° ë©”ëª¨ë¦¬(memory append) + ì´ë¦„ ì¸ì‹ í‘œí˜„ + Clustering ì‹œê°í™” ì¶• ë³€ê²½
2025.04.10 ver 0.0.05 - Memory-Tag ì¶”ê°€ + ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œìŠ¤í…œ + ì¾Œ/ë¶ˆì¾Œ íŒë‹¨(ì†Œë¦¬ë³€í™”ìœ¨) + Clustering ì‹œê°í™” ê°œì„ 
2025.04.11 ver 0.0.06 - í…ìŠ¤íŠ¸ ìƒí˜¸ì‘ìš©(ì…ë ¥ ì „í›„ 10 sec ì†Œë¦¬/ì˜ìƒ í†µí•© ê¸°ì–µ ì €ì¥) + data format ìˆ˜ì • + Lagacy í˜¸í™˜ì„± ìœ ì§€ + Background sound + í…ìŠ¤íŠ¸ ì…ë ¥ ì°½ ë¶„ë¦¬ + ìë™ì €ì¥(í•˜ë£¨ë‹¨ìœ„)


"""



import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import cv2
import pyaudio
import numpy as np
import time
import json
import os
import threading
import queue
import tkinter as tk
from tkinter import simpledialog
from collections import deque
from sklearn.cluster import KMeans


# -----------------------------------------------
# ê¸°ë³¸ ì„¤ì •
# -----------------------------------------------


CAMERA_WIDTH = 160
CAMERA_HEIGHT = 120
MIC_DEVICE_INDEX = 1    # USB ë§ˆì´í¬


CHUNK = 1024
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 44100


EMOTION_THRESHOLD = 0.7


MEMORY_DIR = "Updated_Memory"
memory = []
learning_mode = False
is_learning_session_active = False  # ì „ì—­ ë³€ìˆ˜ë¡œ ì¶”ê°€
NAME_PATTERN_MEMORY = []
data_points = deque(maxlen=1000)
brightness_history = deque(maxlen=5)
volume_history = deque(maxlen=5)
emotion_history = deque(maxlen=100)
important_event_history = deque(maxlen=100)
plot_queue = queue.Queue()
exit_flag = threading.Event()
SAVE_INTERVAL = 1800
last_save_time = time.time()
recent_audio = deque(maxlen=int(RATE / CHUNK * 10))  # 10ì´ˆ ë¶„ëŸ‰ ì˜¤ë””ì˜¤ ë²„í¼
recent_visual = deque(maxlen=300)  # 10ì´ˆ ë¶„ëŸ‰ ë¹„ì£¼ì–¼ í”„ë ˆì„ (ì•½ 30fps ê°€ì •)
background_memory = deque(maxlen=300)  # ìµœê·¼ 30ì´ˆ ì •ë„ ì†Œë¦¬ ì €ì¥


# -----------------------------------------------
# ê¸°ì–µ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì €ì¥í•˜ê¸°
# -----------------------------------------------


def load_memory():
    global memory
    if not os.path.exists(MEMORY_DIR):
        os.makedirs(MEMORY_DIR)
    files = sorted([f for f in os.listdir(MEMORY_DIR) if f.startswith('memory_') and f.endswith('.json')])
    total_memories = []
    for filename in files:
        memory_path = os.path.join(MEMORY_DIR, filename)
        with open(memory_path, 'r') as f:
            data = json.load(f)
            if isinstance(data, dict) and 'memories' in data:
                total_memories.extend(data['memories'])
            else:
                total_memories.extend(data)
    memory = total_memories
    print(f"[Memory Loaded] {len(memory)} memories restored from {len(files)} files.")




def save_memory():
    if not os.path.exists(MEMORY_DIR):
        os.makedirs(MEMORY_DIR)
    today = time.strftime("%Y.%m.%d")
    memory_path = os.path.join(MEMORY_DIR, f"memory_{today}.json")
    data_to_save = {
        "version": "0.0.06",
        "last_updated": today,
        "memories": memory
    }
    with open(memory_path, 'w') as f:
        json.dump(data_to_save, f, indent=4)
    print(f"[Memory Saved] {memory_path}")




# -----------------------------------------------
# ê°ì • ë° ê¸°ì–µ ì²˜ë¦¬
# -----------------------------------------------


def calculate_emotion(brightness_change, volume_rate_of_change):
    emotion_score = 0.0
    if brightness_change > 30 or volume_rate_of_change > 0.3:
        if brightness_change > 50 or volume_rate_of_change > 0.5:
            emotion_score = -1.0
        else:
            emotion_score = 1.0
    return max(-1.0, min(1.0, emotion_score))


def maybe_store_memory(input_summary, emotion_score):
    if abs(emotion_score) >= EMOTION_THRESHOLD:
        memory.append({
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "emotion": "Pleasure" if emotion_score > 0 else ("Displeasure" if emotion_score < 0 else "Neutral"),
            "strength": abs(emotion_score),
            "content": input_summary
        })


# -----------------------------------------------
# ì´ë¦„ ì¸ì‹ ì²˜ë¦¬
# -----------------------------------------------


def update_background_memory(current_fft):
    background_memory.append(current_fft)


def is_background_noise(current_fft):
    if len(background_memory) < 10:
        return False  # ë°ì´í„° ë¶€ì¡±í•˜ë©´ ë¬´ì¡°ê±´ ì´ë²¤íŠ¸ë¡œ ê°„ì£¼
    average_background = np.mean(background_memory, axis=0)
    similarity = np.linalg.norm(current_fft - average_background)
    return similarity < 8000  # Threshold (ì‹¤í—˜í•´ì„œ ì¡°ì • ê°€ëŠ¥)


def update_name_memory(current_fft):
    global NAME_PATTERN_MEMORY
    if len(NAME_PATTERN_MEMORY) < 5:
        NAME_PATTERN_MEMORY.append(current_fft)
    else:
        similarity = np.mean([np.linalg.norm(current_fft - pattern) for pattern in NAME_PATTERN_MEMORY])
        if similarity < 10000:
            print("[SOAR]: Familiar sound detected (possible name recognition)")
            important_event_history.append(1)
            return
    important_event_history.append(0)




# -----------------------------------------------
# í•™ìŠµ ì´ë²¤íŠ¸ í•¨ìˆ˜ (í…ìŠ¤íŠ¸ ì…ë ¥ + ì˜¤ë””ì˜¤/ë¹„ì£¼ì–¼ í†µí•© ì €ì¥)
# -----------------------------------------------


def capture_learning_event():
    global is_learning_session_active
    if is_learning_session_active:
        print("[Learning] ì…ë ¥ ì„¸ì…˜ì´ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        return


    is_learning_session_active = True  # ì„¸ì…˜ ì‹œì‘
    root = tk.Tk()
    root.withdraw()


   


# -----------------------------------------------
# í”Œë¡¯ ìŠ¤ë ˆë“œ
# -----------------------------------------------


def plot_thread_func(q):
    plt.ion()
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    plt.show(block=False)


    while not exit_flag.is_set():
        try:
            while True:
                data = q.get_nowait()
                if data == 'EXIT':
                    exit_flag.set()
                    break


                emotion_history_local, data_points_local, important_event_local = data


                ax1.cla()
                for i, score in enumerate(emotion_history_local):
                    if i < len(important_event_local) and important_event_local[i] == 1:
                        color = 'red' if score < 0 else 'blue'
                    else:
                        color = 'gray'
                    ax1.scatter(i, score, color=color, s=10)
                ax1.set_ylim([-1.2, 1.2])
                ax1.set_title("SOAR Emotion Over Time")
                ax1.set_xlabel("Time (frames)")
                ax1.set_ylabel("Emotion Score")
                ax1.axhline(0, color='gray', linestyle='--')


                if len(data_points_local) >= 5:
                    X = np.array(data_points_local)
                    kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)
                    kmeans.fit(X)
                    labels = kmeans.labels_
                    centers = kmeans.cluster_centers_


                    ax2.cla()
                    for i in range(3):
                        cluster = X[labels == i]
                        ax2.scatter(cluster[:, 0], cluster[:, 1], label=f"Cluster {i}")
                    ax2.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=100, label='Centers')
                    ax2.set_title("SOAR Clustering")
                    ax2.set_xlabel("Brightness Change")
                    ax2.set_ylabel("Dominant Frequency")
                    ax2.legend()


                plt.tight_layout()
                plt.pause(0.001)


        except queue.Empty:
            continue


    plt.close('all')


# -----------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------------------------------


load_memory()


cap = cv2.VideoCapture(1)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)


cv2.namedWindow('SOAR - First Sight', cv2.WINDOW_NORMAL)  # ì°½ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥
cv2.resizeWindow('SOAR - First Sight', 640, 480)  # ì›í•˜ëŠ” ì°½ í¬ê¸°ë¡œ ì„¤ì •


if not cap.isOpened():
    print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()


audio = pyaudio.PyAudio()
stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, input_device_index=MIC_DEVICE_INDEX, frames_per_buffer=CHUNK)


plot_thread = threading.Thread(target=plot_thread_func, args=(plot_queue,))
plot_thread.start()


print("SOAR ê°ê° í™œì„±í™” ì™„ë£Œ. (që¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ)")


try:
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì¹´ë©”ë¼ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break


        frame = cv2.flip(frame, 1)


        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray)
        brightness_history.append(brightness)


        data = stream.read(CHUNK, exception_on_overflow=False)
        audio_data = np.frombuffer(data, dtype=np.int16)
        recent_audio.append(audio_data)
        recent_visual.append(frame)
        volume = np.linalg.norm(audio_data) / CHUNK
        volume_history.append(volume)


        fft_audio = np.fft.fft(audio_data)
        fft_magnitude = np.abs(fft_audio)
        dominant_frequency = np.argmax(fft_magnitude)


        update_background_memory(fft_magnitude)


        if is_background_noise(fft_magnitude):
            important_event_history.append(0)  # ë°°ê²½ ì†Œë¦¬ëŠ” ë¬´ì‹œ
        else:
            update_name_memory(fft_magnitude)  # ì§„ì§œ ì´ë²¤íŠ¸(ì´ë¦„)ì¼ ë•Œë§Œ ì¸ì‹


        if len(brightness_history) >= 2 and len(volume_history) >= 3:
            brightness_change = abs(brightness_history[-1] - brightness_history[-2])
            previous_volume = volume_history[-2]
            if previous_volume != 0:
                volume_rate_of_change = abs((volume_history[-1] - previous_volume) / previous_volume)
            else:
                volume_rate_of_change = 0


            emotion_score = calculate_emotion(brightness_change, volume_rate_of_change)
            emotion_history.append(emotion_score)


            input_summary = f"Brightness Change: {brightness_change:.2f}, Volume Rate of Change: {volume_rate_of_change:.2f}"
            maybe_store_memory(input_summary, emotion_score)


            data_points.append([brightness_change, dominant_frequency])


            if len(data_points) % 5 == 0:
                plot_queue.put((list(emotion_history), list(data_points), list(important_event_history)))


        if time.time() - last_save_time > SAVE_INTERVAL:
            save_memory()
            last_save_time = time.time()
            print("[Auto-Save]: Memory saved.")


        # ğŸ’¥ ì—¬ê¸°!! ìˆ˜ì •ëœ ì…ë ¥ì°½ ê´€ë¦¬
        if learning_mode and not is_learning_session_active:
            is_learning_session_active = True
            root = tk.Tk()
            root.withdraw()


            word = simpledialog.askstring("SOAR í•™ìŠµ", "SOARì—ê²Œ ê°€ë¥´ì¹  ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì·¨ì†Œ ëˆ„ë¥´ë©´ ì¢…ë£Œ)", parent=root)
            if word is not None:
                word = word.strip()
                if word != '':
                    if recent_audio:
                        audio_data = np.concatenate(list(recent_audio))
                        fft_data = np.abs(np.fft.fft(audio_data))
                        audio_pattern = fft_data.tolist()
                    else:
                        audio_pattern = []


                    visual_frames_encoded = []
                    for frame in list(recent_visual):
                        _, buffer = cv2.imencode('.jpg', frame)
                        visual_frames_encoded.append(buffer.tolist())


                    memory_entry = {
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "label": word,
                        "audio_pattern": audio_pattern,
                        "visual_frames": visual_frames_encoded,
                        "memory_importance": 0.5
                    }
                    memory.append(memory_entry)
                    save_memory()
                    print(f"[Learned] '{word}' ë‹¨ì–´ì™€ ê´€ë ¨ëœ íŒ¨í„´ ì €ì¥ ì™„ë£Œ.")


            try:
                root.destroy()
            except:
                pass


            is_learning_session_active = False
            learning_mode = False


        # ğŸ‘‘ ì—¬ê¸°ê¹Œì§€ ì…ë ¥ ì²˜ë¦¬


        cv2.imshow('SOAR - First Sight', frame)


        key = cv2.waitKey(1) & 0xFF


        if key == ord('q'):
            break
        if key == ord('t'):
            learning_mode = True  # <-- í”Œë˜ê·¸ ON






except KeyboardInterrupt:
    print("\nì¢…ë£Œ ìš”ì²­ ê°ì§€.")


plot_queue.put('EXIT')
exit_flag.set()
plot_thread.join()


cap.release()
stream.stop_stream()
stream.close()
audio.terminate()
cv2.destroyAllWindows()
save_memory()


print("SOAR ê°ê° ì‹œìŠ¤í…œ ì¢…ë£Œ.")





